"""
CPR Discriminator for Geo-Flow VLA.

Implements a discriminator for Conditional Policy Regularization (CPR).
The discriminator distinguishes between:
- Real: (state, goal) pairs from the dataset
- Fake: (state, goal) pairs generated by the policy

This regularizes the policy to generate geometrically plausible trajectories.

References:
    - Adversarial training for imitation learning
    - GAIL: Generative Adversarial Imitation Learning
"""

from typing import Dict, Optional, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import Tensor


class SpectralNorm(nn.Module):
    """Wrapper for spectral normalization."""
    
    def __init__(self, module: nn.Module, name: str = 'weight', n_power_iterations: int = 1):
        super().__init__()
        self.module = nn.utils.spectral_norm(module, name, n_power_iterations)
    
    def forward(self, x: Tensor) -> Tensor:
        return self.module(x)


class CPRDiscriminator(nn.Module):
    """
    Discriminator for Conditional Policy Regularization.
    
    Architecture:
        Input: concat(state, goal) → MLP → logit
        
    Trained to output:
        - 1 for real (dataset) pairs
        - 0 for fake (policy-generated) pairs
    
    The policy is regularized by:
        L_CPR = -λ * log(D(state, goal_policy))
    """

    def __init__(
        self,
        state_dim: int = 512,
        latent_dim: int = 256,
        hidden_dim: int = 512,
        num_layers: int = 3,
        dropout: float = 0.1,
        use_spectral_norm: bool = True,
        gradient_penalty_weight: float = 10.0,
    ) -> None:
        """
        Args:
            state_dim: State embedding dimension
            latent_dim: Goal embedding dimension
            hidden_dim: Hidden layer dimension
            num_layers: Number of hidden layers
            dropout: Dropout rate
            use_spectral_norm: Apply spectral normalization for stability
            gradient_penalty_weight: Weight for gradient penalty loss
        """
        super().__init__()
        
        self.state_dim = state_dim
        self.latent_dim = latent_dim
        self.gradient_penalty_weight = gradient_penalty_weight
        
        input_dim = state_dim + latent_dim
        
        # Build discriminator MLP
        layers = []
        dims = [input_dim] + [hidden_dim] * num_layers + [1]
        
        for i in range(len(dims) - 1):
            linear = nn.Linear(dims[i], dims[i + 1])
            
            if use_spectral_norm and i < len(dims) - 2:
                linear = nn.utils.spectral_norm(linear)
            
            layers.append(linear)
            
            if i < len(dims) - 2:  # No activation after last layer
                layers.append(nn.LeakyReLU(0.2))
                layers.append(nn.Dropout(dropout))
        
        self.net = nn.Sequential(*layers)
        
        # Initialize weights
        self._init_weights()

    def _init_weights(self) -> None:
        """Initialize weights for stable training."""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.orthogonal_(m.weight, gain=0.8)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    def forward(self, state: Tensor, goal: Tensor) -> Tensor:
        """
        Compute discriminator logit.
        
        Args:
            state: State embedding (B, state_dim)
            goal: Goal embedding (B, latent_dim)
            
        Returns:
            Discriminator logit (B, 1), not sigmoid-activated
        """
        x = torch.cat([state, goal], dim=-1)
        return self.net(x)

    def get_probability(self, state: Tensor, goal: Tensor) -> Tensor:
        """Get probability that input is real."""
        logit = self.forward(state, goal)
        return torch.sigmoid(logit)

    def compute_loss(
        self,
        real_state: Tensor,
        real_goal: Tensor,
        fake_state: Tensor,
        fake_goal: Tensor,
        label_smoothing: float = 0.1,
    ) -> Dict[str, Tensor]:
        """
        Compute discriminator loss.
        
        Uses binary cross-entropy with optional label smoothing.
        
        Args:
            real_state: Real states from dataset (B, state_dim)
            real_goal: Real goals from dataset (B, latent_dim)
            fake_state: States for policy-generated goals (B, state_dim)
            fake_goal: Policy-generated goals (B, latent_dim)
            label_smoothing: Smooth real labels to 1-ε
            
        Returns:
            Dictionary with losses and metrics
        """
        # Real samples (label = 1)
        real_logit = self.forward(real_state, real_goal)
        real_label = torch.ones_like(real_logit) * (1 - label_smoothing)
        real_loss = F.binary_cross_entropy_with_logits(real_logit, real_label)
        
        # Fake samples (label = 0)
        fake_logit = self.forward(fake_state, fake_goal)
        fake_label = torch.zeros_like(fake_logit) + label_smoothing
        fake_loss = F.binary_cross_entropy_with_logits(fake_logit, fake_label)
        
        # Total discriminator loss
        total_loss = (real_loss + fake_loss) / 2
        
        # Compute gradient penalty if enabled
        gp_loss = torch.tensor(0.0, device=real_state.device)
        if self.gradient_penalty_weight > 0:
            gp_loss = self._gradient_penalty(
                real_state, real_goal,
                fake_state, fake_goal,
            )
            total_loss = total_loss + self.gradient_penalty_weight * gp_loss
        
        # Compute accuracy metrics
        with torch.no_grad():
            real_pred = (real_logit > 0).float()
            fake_pred = (fake_logit > 0).float()
            real_acc = real_pred.mean()
            fake_acc = (1 - fake_pred).mean()
        
        return {
            "loss": total_loss,
            "real_loss": real_loss,
            "fake_loss": fake_loss,
            "gp_loss": gp_loss,
            "real_accuracy": real_acc,
            "fake_accuracy": fake_acc,
        }

    def _gradient_penalty(
        self,
        real_state: Tensor,
        real_goal: Tensor,
        fake_state: Tensor,
        fake_goal: Tensor,
    ) -> Tensor:
        """
        Compute gradient penalty for WGAN-GP style training.
        
        Enforces Lipschitz constraint on discriminator.
        """
        B = real_state.shape[0]
        device = real_state.device
        
        # Random interpolation weight
        alpha = torch.rand(B, 1, device=device)
        
        # Interpolate between real and fake
        interp_state = alpha * real_state + (1 - alpha) * fake_state
        interp_goal = alpha * real_goal + (1 - alpha) * fake_goal
        interp_state.requires_grad_(True)
        interp_goal.requires_grad_(True)
        
        # Compute discriminator output
        interp_logit = self.forward(interp_state, interp_goal)
        
        # Compute gradients
        gradients = torch.autograd.grad(
            outputs=interp_logit,
            inputs=[interp_state, interp_goal],
            grad_outputs=torch.ones_like(interp_logit),
            create_graph=True,
            retain_graph=True,
        )
        
        # Concatenate gradients
        grad_cat = torch.cat([g.view(B, -1) for g in gradients], dim=-1)
        
        # Gradient penalty: (||grad|| - 1)^2
        grad_norm = grad_cat.norm(2, dim=-1)
        gp = ((grad_norm - 1) ** 2).mean()
        
        return gp

    def get_reward(self, state: Tensor, goal: Tensor) -> Tensor:
        """
        Get reward signal for policy (used in CPR loss).
        
        Reward = log(D(state, goal)) - log(1 - D(state, goal))
        
        This is the GAN generator objective.
        
        Args:
            state: State embedding
            goal: Policy-generated goal
            
        Returns:
            Reward signal (B,)
        """
        logit = self.forward(state, goal)
        # Log probability of being classified as real
        reward = F.logsigmoid(logit).squeeze(-1)
        return reward


class TrajectoryDiscriminator(nn.Module):
    """
    Extended discriminator that also considers action trajectories.
    
    Useful for ensuring generated trajectories are geometrically plausible.
    """

    def __init__(
        self,
        state_dim: int = 512,
        latent_dim: int = 256,
        action_dim: int = 7,
        action_horizon: int = 16,
        hidden_dim: int = 512,
        num_layers: int = 3,
        dropout: float = 0.1,
    ) -> None:
        """
        Args:
            state_dim: State embedding dimension
            latent_dim: Goal embedding dimension
            action_dim: Action dimension
            action_horizon: Number of action steps
            hidden_dim: Hidden layer dimension
            num_layers: Number of hidden layers
            dropout: Dropout rate
        """
        super().__init__()
        
        self.action_dim = action_dim
        self.action_horizon = action_horizon
        
        # Action encoder (1D conv over time)
        self.action_encoder = nn.Sequential(
            nn.Conv1d(action_dim, 64, kernel_size=3, padding=1),
            nn.LeakyReLU(0.2),
            nn.Conv1d(64, 128, kernel_size=3, padding=1),
            nn.LeakyReLU(0.2),
            nn.AdaptiveAvgPool1d(1),
            nn.Flatten(),
            nn.Linear(128, hidden_dim // 2),
        )
        
        # Joint discriminator
        input_dim = state_dim + latent_dim + hidden_dim // 2
        
        layers = []
        dims = [input_dim] + [hidden_dim] * num_layers + [1]
        
        for i in range(len(dims) - 1):
            layers.append(nn.Linear(dims[i], dims[i + 1]))
            if i < len(dims) - 2:
                layers.append(nn.LeakyReLU(0.2))
                layers.append(nn.Dropout(dropout))
        
        self.net = nn.Sequential(*layers)

    def forward(
        self,
        state: Tensor,
        goal: Tensor,
        actions: Tensor,
    ) -> Tensor:
        """
        Compute discriminator logit.
        
        Args:
            state: State embedding (B, state_dim)
            goal: Goal embedding (B, latent_dim)
            actions: Action trajectory (B, T, action_dim)
            
        Returns:
            Discriminator logit (B, 1)
        """
        # Encode actions
        actions_t = actions.transpose(1, 2)  # (B, action_dim, T)
        action_feat = self.action_encoder(actions_t)  # (B, hidden//2)
        
        # Concatenate all features
        x = torch.cat([state, goal, action_feat], dim=-1)
        
        return self.net(x)

